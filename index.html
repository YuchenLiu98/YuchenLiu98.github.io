<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yuchen Liu</title>
  
  <meta name="author" content="Yuchen Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/sjtu.png">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Yuchen Liu (刘育辰) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<!-- 		  <img style="vertical-align:middle" src='image/wangtan_name.gif' height='50px' width='WIDTHpx'> -->
		  </name>
        </p>
		<p>Currently, I am a third-year Ph.D. student student at <a href="https://min.sjtu.edu.cn/">MIN Lab</a> in Electrical Engineering Department of Shanghai Jiao Tong University, advised by <a href="https://en.zhiyuan.sjtu.edu.cn/en/faculty/585/detail">Prof. Wenrui Dai</a> and <a href="https://min.sjtu.edu.cn/xhk.htm">Prof. Hongkai Xiong</a>.
		Prior to SJTU, I obtained my bachelor degree at Xi'an Jiao Tong University in 2020.
		</p>
		
		<p>I'm interested in computer vision and deep learning, which include but not limit to Transfer Learning, Self-supervised Learning, Domain Generalization and Face Anti-Spoofing.
		</p>
		<p>Please drop me an email if you are interested in corporation with us.
        </p>


        <p align=center>
          <a href="mailto:liuyuchen6666[at]sjtu.edu.cn">Email</a> &nbsp/&nbsp
<!--           <a href="image/cv_wangtan.pdf">CV</a> &nbsp/&nbsp -->
          <a href="https://scholar.google.com.hk/citations?user=GRcH3nAAAAAJ&hl=zh-CN">Google Scholar</a>  &nbsp/&nbsp
          <a href="https://github.com/YuchenLiu98">Github</a> &nbsp/&nbsp
	  <a href="https://www.researchgate.net/profile/Yuchen_Liu82">ResearchGate</a>

        </p>

        </td>
        <td width="33%">
        <img src="image/yuchen1.jpg" width="180">
        </td>
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
		  <li> <strongsmall>[2023/10]</strongsmall> &nbsp;&nbsp;<smalll>I am invited to serve as a reviewer for CVPR 2024.</smalll><br/>
		  <li> <strongsmall>[2023/10]</strongsmall> &nbsp;&nbsp;<smalll>One paper about multi-modal large language model is released on Arxiv.</smalll><br/>
		  <li> <strongsmall>[2023/07]</strongsmall> &nbsp;&nbsp;<smalll>One paper is accepted by ACMMM 2023.</smalll><br/>
		  <li> <strongsmall>[2023/07]</strongsmall> &nbsp;&nbsp;<smalll>One paper is accepted by ICCV 2023.</smalll><br/>
		  <li> <strongsmall>[2023/02]</strongsmall> &nbsp;&nbsp;<smalll>Two papers are accepted by CVPR 2023.</smalll><br/>
		  <li> <strongsmall>[2023/02]</strongsmall> &nbsp;&nbsp;<smalll>I am invited to serve as a reviewer for ICCV 2023.</smalll><br/>
	          <li> <strongsmall>[2023/02]</strongsmall> &nbsp;&nbsp;<smalll>One paper is accepted by ICASSP 2023.</smalll><br/>
		  <li> <strongsmall>[2022/11]</strongsmall> &nbsp;&nbsp;<smalll>I am invited to serve as a reviewer for CVPR 2023.</smalll><br/>
		  <li> <strongsmall>[2022/07]</strongsmall> &nbsp;&nbsp;<smalll>Two papers are accepted by ECCV 2022.</smalll><br/>
		  <li> <strongsmall>[2022/03]</strongsmall> &nbsp;&nbsp;<smalll>One paper is accepted by ICME 2022 as Oral.</smalll><br/>
		  <li> <strongsmall>[2021/07]</strongsmall> &nbsp;&nbsp;<smalll>One paper is accepted by ICCV 2021.</smalll><br/>
		  
          
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/xjtu.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Xi'an Jiao Tong University (XJTU), Xi'an, China</stronghuge><br />
          Bachelor Degree in Electrical Engineering and Automation &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
          GPA: <strong>93.56</strong>/100, &nbsp;&nbsp;TOFEL: <strong>107</strong>/120, &nbsp;&nbsp;GRE: <strong>335</strong>/340, &nbsp;&nbsp;Ranking: <strong>5/345</strong> <br />
          Supervisor: Prof. <a href="http://www.xjtu.edu.cn/jsnr.jsp?urltype=tree.TreeTempUrl&wbtreeid=1632&wbwbxjtuteacherid=565">Zhibin Pan</a>
          </p>
        </td>
      </tr>
	  
	    <tr>
          <td width="10%">
            <img src='image/sjtu.png' width="110">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Shanghai Jiao Tong University (SJTU), Shanghai, China</stronghuge><br />
          Third-year Ph.D. in <a href="https://mreallab.github.io/">MIN Lab</a>, Department of Electrical Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2020 - Present <br />
          Supervisor: <a href="https://en.zhiyuan.sjtu.edu.cn/en/faculty/585/detail">Prof. Wenrui Dai</a> and <a href="https://min.sjtu.edu.cn/xhk.htm">Prof. Hongkai Xiong</a>
          </p>
        </td>
      </tr>
	  
      </table>





<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="http://cfm.uestc.edu.cn/">
            <img src='image/qualcomm.png' width="115">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Qualcomm University Research Collaboration</stronghuge><br />
          <huge><em>Research Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Nov. 2020 - Nov. 2022 <br />
          Advisors: &nbsp; Prof. <a href="https://scholar.google.com.hk/citations?user=7dy2TtIAAAAJ&hl=zh-CN&oi=ao">Mengran Gou</a> and Prof. <a href="https://www.linkedin.cn/incareer/in/xuan-zou-07565514">Xuan Zou</a>.  <!--
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Combined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.-->
          </p>
        </td>
      </tr>
	      
     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="http://cfm.uestc.edu.cn/">
            <img src='image/Huawei.png' width="90">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Huawei Cloud</stronghuge><br />
          <huge><em>Research</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Nov. 2022 - Now <br />
          </p>
        </td>
      </tr>


<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication <a href="https://scholar.google.com.hk/citations?user=GRcH3nAAAAAJ&hl=zh-CN" style="font-size:22px;">[Google Scholar]</a></heading>
        </td>
      </tr>
      </table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/overall2.pdf'  width="245" height="105">
      </td>
      <td valign="top" width="75%">
	 <strong>From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</strong><br>
	      <a>Dongsheng Jiang*</a>,
     <strong>Yuchen Liu*</strong>,
	<a>Songlin Liu</a>,
	 <a>Xiaopeng Zhang</a>,
	 <a>Jin Li</a>,
	 <a>Hongkai Xiong</a>,
         <a>Qi Tian</a>,
	  &nbsp <br>
        <strong>Arxiv 2023</strong></em><br>
        <em>Area: Multi-modal Large Langugae Model</em> <br>
        <p></p>
<!-- 		<p>We propose a unsupervised domain generalization framework with dual nearest neighbors to unsupervisedly learn domain-invariant features and mitigate the annotation cost of labels for domain generalization.</p> -->
      </td>
    </tr>
   </table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/acm23.png'  width="245" height="105">
      </td>
      <td valign="top" width="75%">
	 <strong>VioLET: Vision-Language Efficient Tuning with Collaborative Multi-modal Gradients</strong><br>
	      <a>Yaoming Wang</a>,
     <strong>Yuchen Liu</strong>,
	 <a>Xiaopeng Zhang</a>,
	 <a>Jin Li</a>,
	 <a>Bowen Shi</a>,
	 <a>Chenglin Li</a>,
 	 <a>Wenrui Dai</a>,
	 <a>Hongkai Xiong</a>,
         <a>Qi Tian</a>,
	  &nbsp <br>
        <em>ACM International Conference on Multimedia, <strong>ACM MM 2023</strong></em><br>
        <em>Area: Parameter-efficient Tuning, Multi-modal Foundation Model</em> <br>
        <p></p>
<!-- 		<p>We propose a unsupervised domain generalization framework with dual nearest neighbors to unsupervisedly learn domain-invariant features and mitigate the annotation cost of labels for domain generalization.</p> -->
      </td>
    </tr>
   </table>
	     
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/iccv23_udgfas.png'  width="245" height="105">
      </td>
      <td valign="top" width="75%">
	 <strong>Towards Unsupervised Domain Generalization for Face Anti-Spoofing</strong><br>
	      <strong>Yuchen Liu</strong>,
     <a>Yabo Chen</a>,
	 <a>Mengran Gou</a>,
	 <a>Chun-Ting Huang</a>,
	 <a>Yaoming Wang</a>,
 	 <a>Wenrui Dai</a>,
	 <a>Hongkai Xiong</a>,

	  &nbsp <br>
        <em>International Conference on Computer Vision, <strong>ICCV 2023</strong></em><br>
	<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Towards_Unsupervised_Domain_Generalization_for_Face_Anti-Spoofing_ICCV_2023_paper.pdf">[Paper]</a>,
	      <a href="https://drive.google.com/file/d/1XsSUJ30iC5WO8fNkf23o8kdP01KUkWYc/view?usp=sharing">[Poster]</a><br>
        <em>Area: Unsupervised Domain Generalization, Face Anti-Spoofing</em> <br>
        <p></p>
<!-- 		<p>We propose a unsupervised domain generalization framework with dual nearest neighbors to unsupervisedly learn domain-invariant features and mitigate the annotation cost of labels for domain generalization.</p> -->
      </td>
    </tr>
   </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/cvpr23_dna.PNG'  width="235" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization</strong><br>
	      <strong>Yuchen Liu</strong>,
     <a>Yaoming Wang</a>,
	 <a>Yabo Chen</a>,     
 	 <a>Wenrui Dai</a>,
	 <a>Chenglin Li</a>,
	 <a>Junni Zou</a>,
	 <a>Hongkai Xiong</a>,

	  &nbsp <br>
        <em>Computer Vision and Pattern Recognition, <strong>CVPR 2023</strong></em><br>
	      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2023_paper.pdf">[Paper]</a>,
	      <a href="https://cvpr2023.thecvf.com/media/PosterPDFs/CVPR%202023/22920.png?t=1684290276.3634677">[Poster]</a>,
	      <a href="https://youtu.be/rryNM9ty1CQ">[Video]</a><br>
        <em>Area: Self-supervised Contrastive Learning, Unsupervised Domain Generalization</em> <br>
        <p></p>
<!-- 		<p>We propose a unsupervised domain generalization framework with dual nearest neighbors to unsupervisedly learn domain-invariant features and mitigate the annotation cost of labels for domain generalization.</p> -->
      </td>
    </tr>
   </table>
	      
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/cvpr23_snf.png'  width="235" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Adapting Shortcut with Normalizing Flow: An Efficient Tuning
Framework for Visual Recognition</strong><br>
     <a>Yaoming Wang</a>,
	      <a>Bowen Shi</a>,
	 <a>Xiaopeng Zhang</a>,     
	 <a>Jin Li</a>, 
	 <strong>Yuchen Liu</strong>,
 	 <a>Wenrui Dai</a>,
	 <a>Chenglin Li</a>,
	 <a>Hongkai Xiong</a>,
	 <a>Qi Tian</a>

	  &nbsp <br>
        <em>Computer Vision and Pattern Recognition, <strong>CVPR 2023</strong></em><br>
	<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adapting_Shortcut_With_Normalizing_Flow_An_Efficient_Tuning_Framework_for_CVPR_2023_paper.pdf">[Paper]</a>,
	      <a href="https://cvpr2023.thecvf.com/media/PosterPDFs/CVPR%202023/22667.png?t=1685626488.6117465">[Poster]</a>,
	      <a href="https://cvpr2023.thecvf.com/media/cvpr-2023/Slides/22667.pdf">[Slide]</a>,
	      <a href="https://youtu.be/NOtnGfYdo0E">[Video]</a><br>
        <em>Area: Parameter-efficient Finetuning</em> <br>
        <p></p>
<!-- 		<p>We propose a unsupervised domain generalization framework with dual nearest neighbors to unsupervisedly learn domain-invariant features and mitigate the annotation cost of labels for domain generalization.</p> -->
      </td>
    </tr>
   </table>
	     
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/icassp.png'  width="235" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Learning Causal Representations for Generalizable Face Anti Spoofing</strong><br>
	      <a>Guanghao Zheng*</a>,
	      <strong>Yuchen Liu*</strong>,   
 	 <a>Wenrui Dai</a>,
	 <a>Chenglin Li</a>,
	 <a>Junni Zou</a>,
	 <a>Hongkai Xiong</a>,

	  &nbsp <br>
        <em>2023 IEEE International Conference on Acoustics, Speech and Signal Processing, <strong>ICASSP 2023</strong></em><br>
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10095329">[Paper]</a><br>
	      <em>Area: Causal Inference, Domain Generalization, Face Anti-Spoofing</em> <br>
	      
        <p></p>
<!-- 		<p>We propose a unsupervised domain generalization framework with dual nearest neighbors to unsupervisedly learn domain-invariant features and mitigate the annotation cost of labels for domain generalization.</p> -->
      </td>
    </tr>
   </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/sda_fas.png'  width="235" height="115">
      </td>
      <td valign="top" width="75%">
	 <strong>Source-Free Domain Adaptation with Contrastive Domain Alignment and Self-supervised Exploration for Face Anti-Spoofing</strong><br>
	 <strong>Yuchen Liu</strong>,
	 <a>Yabo Chen</a>,
	 <a>Wenrui Dai</a>,
 	 <a>Mengran Gou</a>,
	 <a>Chun-Ting Huang</a>,
	 <a>Hongkai Xiong</a>,
	      &nbsp <br>
        <em>European Conference on Computer Vision, <strong>ECCV 2022</strong></em><br>
<!-- 		<em><strong><font color="#a82e2e">(Final Rating: 122)</font></strong></em> <br> -->
		
		<a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19775-8_30.pdf">[Paper]</a>, 
	      <a href="https://github.com/YuchenLiu98/ECCV2022-SDA-FAS">[Code]</a>, 
	      <a href="https://drive.google.com/file/d/1GKBizbnSw1EVzRgMJvIFsoUgMB6lxrOY/view?usp=sharing">[Poster]</a>, 
	      <a href="https://drive.google.com/file/d/1LFWOA6FJ3wjKu5bIz53q-YJcxEmOYxYf/view?usp=sharing">[Video]</a><br>
        <em>Area: Domain Adaptation, Face Anti-Spoofing</em> <br>
        <p></p>
<!-- 		<p>We show why insufficient data renders the model more easily biased to the limited training environment, and propose to impose two "good" inductive biases: equivariance and invariance for robust feature learning.</p> -->
      </td>
    </tr>
   </table>
   
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/sdae.png'  width="235" height="128">
      </td>
      <td valign="top" width="75%">
	 <strong>SdAE: Self-distillated Masked Autoencoder</strong><br>
	 <a>Yabo Chen*</a>,
	 <strong>Yuchen Liu*</strong>,
	 <a>Dongsheng Jiang</a>,
	 <a>Xiaopeng Zhang</a>,
 	 <a>Wenrui Dai</a>,
	 <a>Hongkai Xiong</a>,
 	 <a>Qi Tian</a>, 
        <em>European Conference on Computer Vision, <strong>ECCV 2022</strong></em><br>
<!-- 		<em><strong><font color="#a82e2e">(Final Rating: 122)</font></strong></em> <br> -->
		
		<a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20056-4_7.pdf">[Paper]</a>, 
	      <a href="https://github.com/AbrahamYabo/SdAE">[Code]</a>, 
	      <a href="https://drive.google.com/u/0/uc?id=1VspUjNhBN8hcILGMAWr3kinKQR7jjoT6&export=download">[Poster]</a> 
	      <br>
        <em>Area: Self-supervised Learning</em> <br>
        <p></p>
<!-- 		<p>We show why insufficient data renders the model more easily biased to the limited training environment, and propose to impose two "good" inductive biases: equivariance and invariance for robust feature learning.</p> -->
      </td>
    </tr>
   </table>
   
   

 <!-- , bgcolor="#ffffeb"> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='project/cifas.png'  width="235" height="123">
      </td>
      <td valign="top" width="75%">
	 <strong>Causal Intervention for Generalizable Face Anti-Spoofing</strong><br>
	 <strong>Yuchen Liu</strong>,
	 <a>Yabo Chen</a>,
 	 <a>Wenrui Dai</a>,
	 <a>Chenglin Li</a>,
	 <a>Junni Zou</a>,
	 <a>Hongkai Xiong</a>,
	      &nbsp <br>
        <em>IEEE International Conference on Multimedia and Expo, <strong>ICME 2022 Oral</strong></em><br>
<!-- 		<em><strong><font color="#a82e2e">(Spotlight Presentation, Top 3%); (2022 PREMIA Best Student Paper)</font></strong></em> <br> -->
		
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9859783">[Paper]</a>, 
		<a href="https://docs.google.com/presentation/d/1SqRzmJvydKRSzoyPAyQ5AmobtyyDFcTp/edit?usp=sharing&ouid=105512821148261129875&rtpof=true&sd=true">[Slides]</a>, 
	      <a href="https://drive.google.com/file/d/1Ybwy0Tk57qTIxFFQYkB32uGVMWGVgWOT/view?usp=sharing">[Video]</a><br>
		
        <em>Area: Causal Inference, Domain Generalization, Face Anti-Spoofing</em> <br>
<!--         <p></p>
		<p>We presented an unsupervised disentangled representation learning method called IP-IRM, based on Self-Supervised Learning (SSL). IP-IRM 
		iteratively partitions the dataset into semantic-related subsets, and learns a representation invariant across the subsets using SSL with an IRM loss.</p> -->
      </td>
    </tr>
   </table>
   
   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vim_nas.png'  width="235" height="125">
      </td>
      <td valign="top" width="75%">
	 <strong>Learning Latent Architectural Distribution in Differentiable Neural
Architecture Search via Variational Information Maximization</strong><br>
	      <a>Yaoming Wang*</a>,
     <strong>Yuchen Liu*</strong>,
 	 <a>Wenrui Dai</a>,
	 <a>Chenglin Li</a>,
	 <a>Junni Zou</a>,
	 <a>Hongkai Xiong</a>,

	  &nbsp <br>
        <em>IEEE International Conference on Computer Vision, <strong>ICCV 2021</strong></em><br>
		<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Learning_Latent_Architectural_Distribution_in_Differentiable_Neural_Architecture_Search_via_ICCV_2021_paper.pdf">[Paper]</a>, 
		<a href="https://docs.google.com/presentation/d/1BFITvbekrnowDGg33GTHhVRDfxPfGHi_/edit?usp=sharing&ouid=105512821148261129875&rtpof=true&sd=true">[Poster]</a></em><br>
        <em>Area: Mutual Information, Neural Architecture Search</em> <br>
        <p></p>
<!-- 		<p>We propose a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. In particular, multiple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer.</p> -->
      </td>
    </tr>
   </table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Academic Service</heading>
          <div style="line-height:25px">
          <p>
<!-- 		  <li> <stronghuge>Co-organizer:</stronghuge> &nbsp; <a href="https://nicochallenge.com/">NICO Challenge 2022</a> (ECCV'22 Workshop)<br/>
		  <li> <stronghuge>PC Member:</stronghuge> &nbsp; CVPR'22, ECCV'22, AAAI'23<br/> -->
		  <li> <stronghuge>Conference Reviewer:</stronghuge> &nbsp; ACMMM'22, CVPR'23, ICME'23, ICCV'23<br/>
		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; TPAMI, IJCV<br/>
          </p>
          </div>
        </td>
      </tr>
</table>




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
<!-- 		            <li> <stronghuge>Outstanding Undergraduate Thesis Award</stronghuge> (Top 2% student),&nbsp; 2020<br/> -->
          <li> <stronghuge>National Scholarship</stronghuge> (Top 2% student),&nbsp; 2017, 2018<br/>
		  <li> <stronghuge>ZhangXu Academician Scholarship</stronghuge> (Only 10 students per year in Department),&nbsp; 2020-2025<br/>
<!--           <li>  <stronghuge>Tang Lixin Sponsored Elite Scholarship</stronghuge> (Only 60 awardees pre year in UESTC),&nbsp; 2017 <br/>
          <li> <stronghuge>Best Freshman Award</stronghuge> (Top 1 student per year in Department),&nbsp; 2016<br/>
          <li> <stronghuge>Honor Student Scholarship</stronghuge> (Top 10 students per year in Department),&nbsp; 2018<br/>
		  <li> <stronghuge>Outstanding Student Scholarship</stronghuge> (Top 10% student),&nbsp; 2017~2019<br/> -->
          </p>
          </div>
        </td>
      </tr>
</table>          



<!-- <p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)
          </p>
          <p>
          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.
          </p>
      </td>
      </tr> -->





   <p></p><p></p><p></p><p></p><p></p>
   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=268&t=tt&d=EDoJTbQxXZ1s-RIL0vsAEErw0u8Vc6AMHLjK1Cjx3cY"></script>
<!--    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.png?cl=ffffff&w=268&t=tt&d=EDoJTbQxXZ1s-RIL0vsAEErw0u8Vc6AMHLjK1Cjx3cY"></script> -->
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="left"><font size="2">
				Last updated on July, 2023
				<p align="middle"><font size="2">
				<a href="https://people.eecs.berkeley.edu/~barron/">Template</a>
				</tbody></table>
   

</body>
</html>
