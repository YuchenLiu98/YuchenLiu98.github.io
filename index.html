<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tan Wang</title>
  
  <meta name="author" content="Yuchen Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/sjtu.png">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Yuchen Liu (刘育辰) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<!-- 		  <img style="vertical-align:middle" src='image/wangtan_name.gif' height='50px' width='WIDTHpx'> -->
		  </name>
        </p>
		<p>Currently, I am a third-year Ph.D. student student at <a href="https://min.sjtu.edu.cn/">MIN Lab</a> in Electrical Engineering Department of Shanghai Jiao Tong University, advised by <a href="https://en.zhiyuan.sjtu.edu.cn/en/faculty/585/detail">Prof. Wenrui Dai</a> and <a href="https://min.sjtu.edu.cn/xhk.htm">Prof. Hongkai Xiong</a>.
		Prior to SJTU, I obtained my bachelor degree at Xi'an Jiao Tong University in 2020.
		</p>
		
		<p>I'm interested in computer vision and deep learning, which include but not limit to Transfer Learning, Self-supervised Learning, Domain Generalization and Face Anti-Spoofing.
		</p>
		<p>Please drop me an email if you are interested in corporation with us.
        </p>


        <p align=center>
          <a href="mailto:liuyuchen6666[at]sjtu.edu.cn">Email</a> &nbsp/&nbsp
<!--           <a href="image/cv_wangtan.pdf">CV</a> &nbsp/&nbsp -->
          <a href="https://scholar.google.com.hk/citations?user=GRcH3nAAAAAJ&hl=zh-CN">Google Scholar</a>  &nbsp/&nbsp
          <a href="https://github.com/YuchenLiu98">Github</a> &nbsp/&nbsp
	  <a href="https://www.researchgate.net/profile/Yuchen_Liu82">ResearchGate</a> &nbsp/&nbsp

        </p>

        </td>
        <td width="33%">
        <img src="image/yuchen.png" width="180">
        </td>
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
		  <li> <strongsmall>[2022/07]</strongsmall> &nbsp;&nbsp;<smalll>Two papers are accepted by ECCV 2022.</smalll><br/>
		  <li> <strongsmall>[2022/03]</strongsmall> &nbsp;&nbsp;<smalll>One paper is accepted by ICME 2022 as Oral.</smalll><br/>
		  <li> <strongsmall>[2021/07]</strongsmall> &nbsp;&nbsp;<smalll>One paper is accepted by ICCV 2021.</smalll><br/>
		  
          
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/xjtu.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Xi'an Jiao Tong University (XJTU), China</stronghuge><br />
          Bachelor Degree in Electrical Engineering and Automation &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
          GPA: <strong>93.56</strong>/100, &nbsp;&nbsp;TOFEL: <strong>107</strong>/120, &nbsp;&nbsp;GRE: <strong>335</strong>/340, &nbsp;&nbsp;Ranking: <strong>5/345</strong> <br />
          Supervisor: Prof. <a href="http://www.xjtu.edu.cn/jsnr.jsp?urltype=tree.TreeTempUrl&wbtreeid=1632&wbwbxjtuteacherid=565">Zhibin Pan</a>
          </p>
        </td>
      </tr>
	  
	    <tr>
          <td width="10%">
            <img src='image/sjtu.png' width="110">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Shanghai Jiao Tong University (SJTU), Shanghai</stronghuge><br />
          Third-year Ph.D. in <a href="https://mreallab.github.io/">MIN Lab</a>, Department of Electrical Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2020 - Present <br />
          Supervisor: <a href="https://en.zhiyuan.sjtu.edu.cn/en/faculty/585/detail">Prof. Wenrui Dai</a> and <a href="https://min.sjtu.edu.cn/xhk.htm">Prof. Hongkai Xiong</a>
          </p>
        </td>
      </tr>
	  
      </table>





<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="http://cfm.uestc.edu.cn/">
            <img src='image/qualcomm.png' width="115">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Qualcomm University Research Collaboration</stronghuge><br />
          <huge><em>Research Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Nov. 2020 - Nov. 2022 <br />
          Advisors: &nbsp; Prof. <a href="https://scholar.google.com.hk/citations?user=7dy2TtIAAAAJ&hl=zh-CN&oi=ao">Mengran Gou</a> and Prof. <a href="https://www.linkedin.cn/incareer/in/xuan-zou-07565514">Xuan Zou</a>.  <!--
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Combined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.-->
          </p>
        </td>
      </tr>


<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication <a href="https://scholar.google.com.hk/citations?user=GRcH3nAAAAAJ&hl=zh-CN" style="font-size:22px;">[Google Scholar]</a></heading>
        </td>
      </tr>
      </table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/sda_fas.png'  width="200" height="120">
      </td>
      <td valign="top" width="75%">
	 <strong>Source-Free Domain Adaptation with Contrastive Domain Alignment and Self-supervised Exploration for Face Anti-Spoofing</strong><br>
	 <strong>Yuchen Liu</strong>,
	 <a>Yabo Chen</a>,
	 <a>Wenrui Dai</a>,
 	 <a>Mengran Gou</a>,
	 <a>Chun-Ting Huang</a>,
	 <a>Hongkai Xiong</a>,
        <em>European Conference on Computer Vision, <strong>ECCV 2022</strong></em><br>
		<em><strong><font color="#a82e2e">(Final Rating: 122)</font></strong></em> <br>
		
		<a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19775-8_30.pdf">[Paper]</a>, <a href="https://github.com/YuchenLiu98/ECCV2022-SDA-FAS">[Code]</a>, <a href="https://drive.google.com/file/d/1L72Wzfap4IPq3UzaGV2qjHbimKMSZymu/view?usp=sharing">[Poster]</a>, <a href="https://drive.google.com/file/d/1ZU4M_ljNEAnRRA-LVNxx5vCXpXF5Iywl/view?usp=sharing">[Slides]</a><br>
        <em>Area: Domain Adaptation, Face Anti-Spoofing</em> <br>
        <p></p>
<!-- 		<p>We show why insufficient data renders the model more easily biased to the limited training environment, and propose to impose two "good" inductive biases: equivariance and invariance for robust feature learning.</p> -->
      </td>
    </tr>
   </table>
   
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/sdae.png'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>SdAE: Self-distillated Masked Autoencoder</strong><br>
	 <a>Yabo Chen*</a>,
	 <strong>Yuchen Liu*</strong>,
	 <a>Dongsheng Jiang</a>,
	 <a>Xiaopeng Zhang</a>,
 	 <a>Wenrui Dai</a>,
	 <a>Hongkai Xiong</a>,
 	 <a>Qi Tian</a>, 
        <em>European Conference on Computer Vision, <strong>ECCV 2022</strong></em><br>
		<em><strong><font color="#a82e2e">(Final Rating: 122)</font></strong></em> <br>
		
		<a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19775-8_30.pdf">[Paper]</a>, <a href="https://github.com/YuchenLiu98/ECCV2022-SDA-FAS">[Code]</a>, <a href="https://drive.google.com/file/d/1L72Wzfap4IPq3UzaGV2qjHbimKMSZymu/view?usp=sharing">[Poster]</a>, <a href="https://drive.google.com/file/d/1ZU4M_ljNEAnRRA-LVNxx5vCXpXF5Iywl/view?usp=sharing">[Slides]</a><br>
        <em>Area: Self-supervised Learning</em> <br>
        <p></p>
<!-- 		<p>We show why insufficient data renders the model more easily biased to the limited training environment, and propose to impose two "good" inductive biases: equivariance and invariance for robust feature learning.</p> -->
      </td>
    </tr>
   </table>
   
   

 <!-- , bgcolor="#ffffeb"> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='project/cifas.png'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>Causal Intervention for Generalizable Face Anti-Spoofing</strong><br>
	 <strong>Yuchen Liu</strong>,
	 <a>Yabo Chen</a>,
 	 <a>Wenrui Dai</a>,
	 <a>Chenglin Li</a>,
	 <a>Junni Zou</a>,
	 <a>Hongkai Xiong</a>,
        <em>IEEE International Conference on Multimedia and Expo, <strong>ICME 2022 Oral</strong></em><br>
		<em><strong><font color="#a82e2e">(Spotlight Presentation, Top 3%); (2022 PREMIA Best Student Paper)</font></strong></em> <br>
		
		<a href="https://arxiv.org/abs/2110.15255">[Paperlink]</a>, 
		<a href="https://github.com/Wangt-CN/IP-IRM">[Code]</a>, <a href="https://drive.google.com/file/d/1ETPfpmttHwzuTs98LhAQMKObVJOhUq2U/view?usp=sharing">[Poster]</a>, <a href="https://drive.google.com/file/d/1LUSnsPFMyEGc4S0BPc6ZbmIhC_UMgr-s/view?usp=sharing">[Slides]</a>, <a href="https://zhuanlan.zhihu.com/p/483952577">[知乎]</a><br>
		
        <em>Area: Self-supervised Representation Learning, Group Theory, Invariant Risk Minimization</em> <br>
        <p></p>
		<p>We presented an unsupervised disentangled representation learning method called IP-IRM, based on Self-Supervised Learning (SSL). IP-IRM 
		iteratively partitions the dataset into semantic-related subsets, and learns a representation invariant across the subsets using SSL with an IRM loss.</p>
      </td>
    </tr>
   </table>
   
   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/caam/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Causal Attention for Unbiased Visual Recognition</strong><br>
     <strong>Tan Wang</strong>,
	 <a href="https://scholar.google.com/citations?user=QeSoG3sAAAAJ&hl">Chang Zhou</a>,
	 <a href="https://qianrusun.com/">Qianru Sun</a>,
     <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,

	  &nbsp <br>
        <em>IEEE International Conference on Computer Vision, <strong>ICCV 2021</strong></em><br>
		<a href="https://arxiv.org/abs/2108.08782">[Paperlink]</a>, 
		<a href="https://github.com/Wangt-CN/CaaM">[Code]</a>, <a href="https://drive.google.com/file/d/1BpT6xPYDKHArqZxthL_98de92mxDkaBd/view?usp=sharing">[Poster]</a>, <a href="https://drive.google.com/file/d/1IZf_DmGX4S06RCVoxLLnR1yLcqvMnIYA/view?usp=sharing">[Slides]</a></em><br>
        <em>Area: Invariant Risk Minimization, OOD Generalization</em> <br>
        <p></p>
		<p>We propose a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. In particular, multiple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer.</p>
      </td>
    </tr>
   </table>
   
   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/gcm_cf/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Counterfactual Zero-Shot and Open-Set Visual Recognition</strong><br>
	 <a>Zhongqi Yue*</a>,
     <strong>Tan Wang*</strong>,
     <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,
     <a href="https://qianrusun.com/">Qianru Sun</a>,
	 <a href="https://scholar.google.com/citations?user=6G-l4o0AAAAJ&hl=en">Xian-sheng Hua</a> &nbsp (* equal contribution)<br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2021</strong></em><br>
		<a href="https://arxiv.org/abs/2103.00887">[Paperlink]</a>, 
		<a href="https://github.com/yue-zhongqi/gcm-cf">[Code]</a>, <a href="https://arxiv.org/abs/2103.00887">[知乎]</a><br>
        <em>Area: Counterfacual, Zero-shot Learning, Open-set Recognition</em> <br>
        <p></p>
		<p>We presented a novel counterfactual framework "Generative Causal Model" for Zero-Shot Learning (ZSL) and Open-Set Recognition (OSR) to provide a theoretical ground for balancing and improving the seen/unseen classification imbalance.</p>
      </td>
    </tr>
   </table>
   
   

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='project/vc-rcnn/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Visual Commonsense R-CNN</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2020</strong></em><br>
		<a href="https://arxiv.org/abs/2002.12204">[Paperlink]</a>, 
		<a href="https://github.com/Wangt-CN/VC-R-CNN">[Code]</a>, <a href="https://zhuanlan.zhihu.com/p/111306353">[知乎]</a></em><br>
        <em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
        <p></p>
		<p>In this paper, we present a novel un-/self-supervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for Vision & Language high-level tasks. </p>
      </td>
    </tr>
   </table>


 <!-- , bgcolor="#ffffeb"> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/VC_CVPRW.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
	 <strong>Visual Commonsense Representation Learning via Causal Inference (Abstact Version of VC R-CNN)</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition MVM Workshop, <strong>CVPRW 2020</strong></em><br>
		<em><strong><font color="#a82e2e">(Oral Presentation)</font></strong></em> <br>
		<a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w26/Wang_Visual_Commonsense_Representation_Learning_via_Causal_Inference_CVPRW_2020_paper.html">[Paperlink]</a>, <a href="https://github.com/Wangt-CN/VC-R-CNN">[Code]</a>, <a href="https://zhuanlan.zhihu.com/p/111306353">[知乎]</a><br>
        
		<em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
      </td>
    </tr>
   </table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='image/mtfn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
	 <strong>Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking</strong><br>
       <strong>Tan Wang</strong>,
    <a href="https://interxuxing.github.io/">Xing Xu</a>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
	  
        <em>ACM International Conference on Multimedia, <strong>MM 2019</strong></em><br>
		<em><strong><font color="#a82e2e">(Oral Presentation, 4.96% acceptance rate)</font></strong></em> <br>
		<a href="https://arxiv.org/abs/1908.04011">[Paperlink]</a>, <a href="https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code">[Code]</a></em><br>
        
        <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable matching performance with acceptable model complexity and much less time consuming. </p>
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/CASC1.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">

 
	 
      <strong>Cross-Modal Attention with Semantic Consistence for Image-Text Matching</strong><br>
	  <a href="https://interxuxing.github.io/">Xing Xu*</a>,
      <strong>Tan Wang*</strong>,
	  <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
	  Lin Zuo,
      <a href="http://cfm.uestc.edu.cn/~fshen/">Fumin Shen</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> &nbsp (* equal contribution) <br>
	      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
          <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel hybrid matching approach named Cross-modal Attention with Semantic Consistence (CASC) for image-text matching, which is a joint framework that performs cross-modal attention for local alignment and multi-label prediction for global semantic consistence.</p>
        <!-- <p . </p> -->
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/radial-gcn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
	  <strong>Radial Graph Convolutional Network for Visual Question Generation</strong><br>     
    <a href="https://interxuxing.github.io/">Xing Xu*</a>,
	<strong>Tan Wang*</strong>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> &nbsp (* equal contribution) <br>
      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
      <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>We propose an innovative answer-centric approach termed Radial Graph Convolutional Network (Radial-GCN) to focus on the relevant image regions only to reduce the complexity on VQG task.</p>
      </td>
    </tr>
   </table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Academic Service</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Co-organizer:</stronghuge> &nbsp; <a href="https://nicochallenge.com/">NICO Challenge 2022</a> (ECCV'22 Workshop)<br/>
		  <li> <stronghuge>PC Member:</stronghuge> &nbsp; CVPR'22, ECCV'22, AAAI'23<br/>
		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; IEEE TNNLS, ACM ToMM<br/>
          </p>
          </div>
        </td>
      </tr>
</table>




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Google PhD Fellowship</stronghuge>,&nbsp; 2022<br/>
		  <li> <stronghuge>2022 PREMIA Best Student Paper Award (The Gold Award)</stronghuge>,&nbsp; 2022<br/>
		  <li> <stronghuge>Jury Prize in ICCV 2021 VIPriors Challenge</stronghuge>,&nbsp; 2021<br/>
		  <li> <stronghuge>NTU Research Scholarship</stronghuge>,&nbsp; 2020<br/>
		  <li> <stronghuge>Outstanding Graduates of Sichuan Province</stronghuge> (Top 1% student),&nbsp; 2020 &nbsp<a href="https://mp.weixin.qq.com/s/GEXwBLLaP3qCM3bZCx1kXw">[Press Coverage]</a><br/>
          <li> <stronghuge>Outstanding Undergraduate Thesis Award</stronghuge> (Top 2% student),&nbsp; 2020<br/>
          <li> <stronghuge>National Scholarship</stronghuge> (Top 2% student),&nbsp; 2017, 2018<br/>
          <li>  <stronghuge>Tang Lixin Sponsored Elite Scholarship</stronghuge> (Only 60 awardees pre year in UESTC),&nbsp; 2017 <br/>
          <li> <stronghuge>Best Freshman Award</stronghuge> (Top 1 student per year in Department),&nbsp; 2016<br/>
          <li> <stronghuge>Honor Student Scholarship</stronghuge> (Top 10 students per year in Department),&nbsp; 2018<br/>
		  <li> <stronghuge>Outstanding Student Scholarship</stronghuge> (Top 10% student),&nbsp; 2017~2019<br/>
          </p>
          </div>
        </td>
      </tr>
</table>          



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)
          </p>
          <p>
          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.
          </p>
      </td>
      </tr>





   <p></p><p></p><p></p><p></p><p></p>
   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=tt&d=85Rlf3OqLYVhTE6hGEcHnAsDJl6O0EsUp326ZMpLzCI"></script>
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="left"><font size="2">
				Last updated on Oct, 2022
				<p align="middle"><font size="2">
				This awesome template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~
				</tbody></table>
   

</body>
</html>
